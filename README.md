<div align="center">

# ğŸŒŸ **MoodLyft Mirror** ğŸŒŸ  
### *Elevating your mood with intelligent emotion detection*

![Build Passing](https://img.shields.io/badge/build-passing-success?style=flat-square)
![Python](https://img.shields.io/badge/python-v3.11-blue?style=flat-square)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/blob/main/CONTRIBUTING.md)
[![License: MIT](https://custom-icon-badges.herokuapp.com/github/license/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?logo=law&logoColor=white)](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/blob/main/LICENSE)
![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20Windows-brightgreen?style=flat-square)
![Views](https://hits.dwyl.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer.svg)
![â­ GitHub stars](https://img.shields.io/github/stars/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?style=social)
![ğŸ´ GitHub forks](https://img.shields.io/github/forks/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?style=social)
![Commits](https://badgen.net/github/commits/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ› GitHub issues](https://img.shields.io/github/issues/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ“‚ GitHub pull requests](https://img.shields.io/github/issues-pr/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ’¾ GitHub code size](https://img.shields.io/github/languages/code-size/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)

</div>

---

## **ğŸ“± What is MoodLyft Mirror?**

The **MoodLyft Mirror** is an advanced emotion detection project that leverages AI to:
- Recognize emotions in real-time through facial analysis.
- Provide uplifting and personalized compliments based on detected emotions.
- Utilize a sleek and modern UI to enhance user experience.

> *"Enhance your day by visualizing and understanding your emotions!"*

---

## **ğŸ“š Table of Contents**
1. [âœ¨ Features](#-features)
2. [ğŸ¦¾ Tech Stack](#-tech-stack)
3. [ğŸ“¸ Screenshots](#-screenshots)
4. [ğŸ§© Try the App](#-try-the-app)
5. [ğŸ‘¨â€ğŸ”§ Setup Instructions](#-setup-instructions)
6. [ğŸ¯ Target Audience](#-target-audience)
7. [ğŸ¤ Contributing](#-contributing)
8. [ğŸ“œ License](#-license)

---

## **âœ¨ Features**  

### **Emotion Detection**
- Real-time emotion recognition using advanced AI algorithms.
- Displays dominant emotions like happiness, sadness, anger, and more.

### **Personalized Compliments**
- Intelligent compliments tailored to your mood.
- Text-to-speech (TTS) functionality to deliver compliments audibly.

### **Modern UI**
- Sleek, frosted-glass design with animated effects.
- Dynamic FPS display for smooth performance insights.

---

## **ğŸ¦¾ Tech Stack**

### ğŸŒ **Core Technologies**
- **Python**: Core programming language.
- **OpenCV**: For real-time video processing and face detection.
- **FER**: Facial Expression Recognition library for emotion analysis.
- **Pillow**: For enhanced text rendering and UI effects.

### **Additional Libraries**
- **Pyttsx3**: For TTS functionality.
- **NumPy**: For numerical operations and efficient data processing.

---

## **ğŸ“¸ Screenshots**
<div align="center">

<table>
  <!-- Screenshot 1 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 1" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 1" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 1</b></td>
    <td><b>Output 1</b></td>
  </tr>
  
  <!-- Screenshot 2 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 2" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 2" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 2</b></td>
    <td><b>Output 2</b></td>
  </tr>
  
  <!-- Screenshot 3 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 3" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 3" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 3</b></td>
    <td><b>Output 3</b></td>
  </tr>
  
  <!-- Screenshot 4 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 4" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 4" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 4</b></td>
    <td><b>Output 4</b></td>
  </tr>
  
  <!-- Screenshot 5 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 5" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 5" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 5</b></td>
    <td><b>Output 5</b></td>
  </tr>
  
  <!-- Screenshot 6 -->
  <tr>
    <td><img src="https://via.placeholder.com/250" alt="Input 6" width="250px"></td>
    <td><img src="https://via.placeholder.com/250" alt="Output 6" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 6</b></td>
    <td><b>Output 6</b></td>
  </tr>
</table>

</div>

---

## **ğŸ§© Try the App**

<div align="center">

### **Want to Experience MoodLyft Mirror?**

Clone the repository and follow the setup instructions to run the project locally.  
Stay tuned for future releases!

</div>

---

## **ğŸ‘¨â€ğŸ”§ Setup Instructions**

### **Prerequisites**
- Python 3.11 or higher installed on your system.
- A webcam for real-time emotion detection.
- Install required Python packages listed in `requirements.txt`.

### **Steps to Run the Project**
1. **Clone the Repository**
   ```bash
   git clone https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer.git
   cd MoodLyft-Mirror
   ```

2. **Set Up a Virtual Environment**
    Setting up a virtual environment ensures that your project's dependencies are isolated from your global Python installation, preventing version conflicts and promoting a clean development environment.

   *For macOS/Linux*
   1. **Create a virtual environment:**
     ```bash
    python3 -m venv moodlyft_env
    ```

   2. **Activate the virtual environment:**
     ```bash
    source moodlyft_env/bin/activate
    ```
   *For Windows*
   1. **Create a virtual environment:**
     ```bash
    python3 -m venv moodlyft_env
    ```

   2. **Activate the virtual environment:**
     ```bash
    moodlyft_env\Scripts\activate 
    ```
3. **Install Dependencies**
   *For macOS/Linux*
     ```bash
    pip install -r requirements-macos.txt
    ```

   *For Windows*
     ```bash
    pip install -r requirements-windows.txt
    ```

4. **Run the Application**
   ```bash
   python main.py
   ```

5. **Experience the App**
   - Ensure your webcam is connected and accessible.
   - Follow on-screen instructions and enjoy!

---

## **ğŸ¯ Target Audience**

1. **Individuals**: Track your mood and uplift your spirits daily.
2. **Therapists**: Utilize emotion detection as part of therapy sessions.
3. **Developers**: Enhance and expand the project with additional features.

---

## **ğŸ¤ Contributing**

We â¤ï¸ open source! Contributions are welcome to make this project even better.  

1. Fork the repository.  
2. Create your feature branch.  
   ```bash
   git checkout -b feature/new-feature
   ```
3. Commit your changes.  
   ```bash
   git commit -m "Add a new feature"
   ```
4. Push to the branch and open a pull request.

---

## **ğŸ“œ License**

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

<div align="center">

### ğŸ“¬ **Feedback & Suggestions**
*We value your input! Share your thoughts through [GitHub Issues](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/issues).*

ğŸ’¡ *Let's work together to uplift emotions and create positivity!*

</div>

<div align="center">

# ğŸŒŸ **MoodLyft Mirror Input Emotion Analyzer** ğŸŒŸ  
### *Elevating your mood with intelligent emotion detection*

![Build Passing](https://img.shields.io/badge/build-passing-success?style=flat-square)
![Python](https://img.shields.io/badge/python-v3.11-blue?style=flat-square)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat-square)](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/blob/main/CONTRIBUTING.md)
[![License: MIT](https://custom-icon-badges.herokuapp.com/github/license/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?logo=law&logoColor=white)](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/blob/main/LICENSE)
![Platform](https://img.shields.io/badge/platform-macOS%20%7C%20Windows-brightgreen?style=flat-square)
![Views](https://hits.dwyl.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer.svg)
![â­ GitHub stars](https://img.shields.io/github/stars/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?style=social)
![ğŸ´ GitHub forks](https://img.shields.io/github/forks/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer?style=social)
![Commits](https://badgen.net/github/commits/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ› GitHub issues](https://img.shields.io/github/issues/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ“‚ GitHub pull requests](https://img.shields.io/github/issues-pr/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)
![ğŸ’¾ GitHub code size](https://img.shields.io/github/languages/code-size/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer)

</div>

---

## **ğŸ“± What is MoodLyft Mirror Input Emotion Analyzer?**

The **MoodLyft Mirror Input Emotion Analyzer** is a comprehensive emotion detection tool that leverages AI to:
- **Analyze Emotions in Media:** Detect emotions in static images and videos by processing files placed in designated input folders.
- **Provide Uplifting and Personalized Compliments:** Generate tailored compliments based on detected emotions to enhance user experience.
- **Organized Input/Output Workflow:** Easily manage your media by placing inputs in specific directories and accessing analyzed outputs effortlessly.

> *"Enhance your day by visualizing and understanding your emotions through your media!"*

---

## **ğŸ“š Table of Contents**
1. [âœ¨ Features](#-features)
2. [ğŸ¦¾ Tech Stack](#-tech-stack)
3. [ğŸ“¸ Screenshots](#-screenshots)
4. [ğŸ§© Try the App](#-try-the-app)
5. [ğŸ‘¨â€ğŸ”§ Setup Instructions](#-setup-instructions)
6. [ğŸ¯ Target Audience](#-target-audience)
7. [ğŸ¤ Contributing](#-contributing)
8. [ğŸ“œ License](#-license)

---

## **âœ¨ Features**  

### **Emotion Detection**
- **Analyze Emotions in Media:** Detect emotions in uploaded images and videos using advanced AI algorithms.
- **Displays Dominant Emotions:** Identify and display emotions such as happiness, sadness, anger, and more.

### **Personalized Compliments**
- **Tailored Uplifting Messages:** Generate intelligent compliments based on the detected emotions.
- **Text-to-Speech (TTS) Functionality:** Deliver compliments audibly for an enhanced interactive experience.

### **Organized Input/Output Workflow**
- **Structured Directories:** Place your input media in designated folders and retrieve analyzed outputs from organized directories.
- **Seamless Processing:** Easily manage and process multiple media files with minimal effort.

---

## **ğŸ¦¾ Tech Stack**

### ğŸŒ **Core Technologies**
- **Python**: Core programming language.
- **OpenCV**: For real-time video processing and face detection.
- **FER**: Facial Expression Recognition library for emotion analysis.
- **Pillow**: For enhanced text rendering and UI effects.

### **Additional Libraries**
- **Pyttsx3**: For TTS functionality.
- **NumPy**: For numerical operations and efficient data processing.

---

## **ğŸ“¸ Screenshots**
<div align="center">

<table>
  
  <!-- Screenshot 1 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/1056690b-3414-4f5f-94b4-8b740aa90c38" alt="Input 1" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/ce75a909-7916-4e31-b505-e15517cd703c" alt="Output 1" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 1</b></td>
    <td><b>Output 1</b></td>
  </tr>
    
  <!-- Screenshot 2 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/27d8e92a-47a5-4e45-9c4a-7e28a6ed365b" alt="Input 2" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/6f5285cb-428e-4072-9b4a-5b3594895b68" alt="Output 2" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 2</b></td>
    <td><b>Output 2</b></td>
  </tr>
  
  <!-- Screenshot 3 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/8ce43db1-fd20-4e8f-847f-56557eb9f139" alt="Input 3" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/ff3747e8-cf90-428f-bdbb-18c653c75de0" alt="Output 3" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 3</b></td>
    <td><b>Output 3</b></td>
  </tr>
  
  <!-- Screenshot 4 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/44e9f332-3d5e-4915-b873-7ac53249514c" alt="Input 4" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/ef05b5ba-c61e-4134-8950-52eeed57645d" alt="Output 4" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 4</b></td>
    <td><b>Output 4</b></td>
  </tr>
  
  <!-- Screenshot 5 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/9576ab20-969d-4323-9520-b91903a11369" alt="Input 5" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/c341d2ee-b800-46b6-98fa-3c54f24ca3ec" alt="Output 5" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 5</b></td>
    <td><b>Output 5</b></td>
  </tr>
  
  <!-- Screenshot 6 -->
  <tr>
    <td><img src="https://github.com/user-attachments/assets/1ba2a25e-214f-4798-ae12-98aa3acb590f" alt="Input 6" width="250px"></td>
    <td><img src="https://github.com/user-attachments/assets/9980a2f2-812e-42d2-a2e0-7675e184f185" alt="Output 6" width="250px"></td>
  </tr>
  <tr>
    <td><b>Input 6</b></td>
    <td><b>Output 6</b></td>
  </tr>
</table>

</div>

---

## **ğŸ§© Try the App**

<div align="center">

### **Want to Experience MoodLyft Mirror?**

Clone the repository and follow the setup instructions to run the project locally.  
Stay tuned for future releases!

</div>

---

## **ğŸ‘¨â€ğŸ”§ Setup Instructions**

### **Prerequisites**
- Python 3.11 or higher installed on your system.
- A webcam for real-time emotion detection.
- Install required Python packages listed in `requirements.txt`.

### **Steps to Run the Project**
1. **Clone the Repository**
   ```bash
   git clone https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer.git
   cd MoodLyft-Mirror-Input-Emotion-Analyzer
   cd MoodLyft-Mirror-Input-Emotion-Analyzer
   ```

2. **Set Up a Virtual Environment**
    Setting up a virtual environment ensures that your project's dependencies are isolated from your global Python installation, preventing version conflicts and promoting a clean development environment.

   *For macOS/Linux*
   1. **Create a virtual environment:**
     ```bash
    python3 -m venv moodlyft_env
    ```

   2. **Activate the virtual environment:**
     ```bash
    source moodlyft_env/bin/activate
    ```
   *For Windows*
   1. **Create a virtual environment:**
     ```bash
    python3 -m venv moodlyft_env
    ```

   2. **Activate the virtual environment:**
     ```bash
    moodlyft_env\Scripts\activate 
    ```
3. **Install Dependencies**
   *For macOS/Linux*
     ```bash
    pip install -r requirements-macos.txt
    ```

   *For Windows*
     ```bash
    pip install -r requirements-windows.txt
    ```

4. **Add Your Media**
- **Images:** Place your input images in the `Input/Images` directory.
- **Videos:** Place your input videos in the `Input/Videos` directory.

5. **Run the Application**
   ```bash
   python main.py
   ```

6. **View Results**
   - Processed images will be saved in Output/analyzedImages.
   - Processed videos will be saved in Output/analyzedVideos.

---

## **ğŸ¯ Target Audience**

1. **Individuals**: Track your mood and uplift your spirits daily.
2. **Therapists**: Utilize emotion detection as part of therapy sessions.
3. **Developers**: Enhance and expand the project with additional features.

---

## **ğŸ¤ Contributing**

We â¤ï¸ open source! Contributions are welcome to make this project even better.  

1. Fork the repository.  
2. Create your feature branch.  
   ```bash
   git checkout -b feature/new-feature
   ```
3. Commit your changes.  
   ```bash
   git commit -m "Add a new feature"
   ```
4. Push to the branch and open a pull request.

---

## **ğŸ“œ License**

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

<div align="center">

### ğŸ“¬ **Feedback & Suggestions**
*We value your input! Share your thoughts through [GitHub Issues](https://github.com/alienx5499/MoodLyft-Mirror-Input-Emotion-Analyzer/issues).*

ğŸ’¡ *Let's work together to uplift emotions and create positivity!*

</div>
